#!/bin/bash

# BoostSRL-Hacksaw v0.01
# Written by Alexander L. Hayes
# hayesall@indiana.edu
# Last updated: 4/25/2017

# Sample calls:
# $ bash hacksaw v1-0.jar
# $ ./hacksaw v1-0.jar
# To refresh static models (this will replace the test results in static/):
# $ ./hacksaw v1-0.jar true

RED='\033[0;31m'
LIGHTGREEN='\033[1;32m'
YELLOW='\033[1;33m'
LIGHTBLUE='\033[1;34m'
NC='\033[0m'
BOLD='\e[1m'
ITAL='\e[3m'
UNDL='\e[4m'

NUMBEROFERRORS=0

function deleteIfExists {
    if [[ -e "$1" ]]; then
	rm -f "$1"; fi
}

function name {
    printf "${BOLD}NAME${NC}\n"
    printf "       hacksaw - BoostSRL Hacksaw: check for consistency between versions of BoostSRL.\n\n"
}

function synopsis {
    printf "${BOLD}SYNOPSIS${NC}\n"
    printf "       bash hacksaw [${UNDL}OPTIONS${NC}]... [${UNDL}JAR${NC}]\n\n"
}

function hlp {
    name; synopsis;
}

if [[ ! -z $1 ]]; then
    JAR=$1
    if [[ ! -e $JAR ]]; then
	printf "${RED}A jar file was specified, but could not be found, here is the help menu:${NC}\n"
    fi
else
    printf "${RED}A jar file was not specified, here is the help menu:${NC}\n"
    hlp
    exit 2
fi

EPOCHS=10

# force moves to occur
UPDATE='false'
if [[ $2 = 'true' ]]; then
    EPOCHS=1
    UPDATE='true'
fi

# add an additional parameter to compare.py for changing the sensitivity threshold

deleteIfExists Results.txt
deleteIfExists INFERENCE.txt
deleteIfExists LEARNING.txt
deleteIfExists roc.txt
deleteIfExists pr.txt

#rm -rf datasets/Father/models/*

# Stretched out:
#"SET='Father';
#TRAIN='-train datasets/Father/train/ -target father';
#TEST='-model datasets/Father/train/models -test datasets/Father/test/ -target father';
#TARGET='father';
#TREES='-trees 25'"

# a hack to create something similar to dictionaries
DATASETS=(
    #"SET='Father'; TRAIN='-train datasets/Father/train/ -target father'; TEST='-model datasets/Father/train/models/ -test datasets/Father/test/ -target father'; TARGET='father'; TREES='-trees 10'"
    #"SET='Toy-Cancer'; TRAIN='-train datasets/Toy-Cancer/train/ -target cancer'; TEST='-model datasets/Toy-Cancer/train/models/ -test datasets/Toy-Cancer/test/ -target cancer'; TARGET='cancer'"
    "SET='WebKB'; TRAIN='-train datasets/WebKB/train/ -target faculty'; TEST='-model datasets/WebKB/train/models/ -test datasets/WebKB/test/ -target faculty'; TARGET='faculty'; TREES='-trees 10'"
    #"SET='IMDB'; TRAIN='-train datasets/IMDB/train/ -target female_gender'; TEST='-model datasets/IMDB/train/models/ -test datasets/IMDB/test/ -target female_gender'; TARGET='female_gender'; TREES='-trees 10'"
    #"SET='Cora'; TRAIN='-train datasets/Cora/train/ -target sameauthor'; TEST='-model datasets/Cora/train/models/ -test datasets/Cora/test/ -target sameauthor'; TARGET='sameauthor'; TREES='-trees 10'"
)

AUCJARPATH='-aucJarPath .'

for ((e=0; e<${EPOCHS}; e++)); do
    echo -e "------\nEpoch $e\n" >> Results.txt
    
    for ((d=0; d<${#DATASETS[*]}; d++)); do
	eval ${DATASETS[d]}
	#echo $SET
	#echo $TRAIN
	#echo $TEST
	#echo $TARGET
	#echo $TREES

	# remove precomputed/recomputed if it was necessary in the previous iteration.
	deleteIfExists precomputed.txt
	deleteIfExists recomputed.txt

	NAMES=(
	    "RDN-Boost"
	    #"RDN-Boost with -noBoost"
	    #"Soft Margin with alpha(0.5) and beta(-2)"
	    #"Soft Margin with alpha(2) and beta(-10)"
	    #"MLN-Boost"
	    #"MLN-Boost with -mlnClause"
	    #"LSTree Boosting Regression"
	)
	
	TRAINS=(
	    "java -jar $JAR -l $TRAIN $TREES"
	    #"java -jar $JAR -l -noBoost $TRAIN"
	    #"java -jar $JAR -l -softm -alpha 0.5 -beta -2 $TRAIN $TREES"
	    #"java -jar $JAR -l -softm -alpha 2 -beta -10 $TRAIN $TREES"
	    #"java -jar $JAR -l -mln $TRAIN $TREES"
	    #"java -jar $JAR -l -mln -mlnClause $TRAIN $TREES"
	    #"java -jar $JAR -l -reg $TRAIN $TREES"
	)
	
	TESTS=(
	    "java -jar $JAR -i $TEST $AUCJARPATH $TREES"
	    #"java -jar $JAR -i $TEST $AUCJARPATH"
	    #"java -jar $JAR -i $TEST $AUCJARPATH $TREES"
	    #"java -jar $JAR -i $TEST $AUCJARPATH $TREES"
	    #"java -jar $JAR -i $TEST $AUCJARPATH $TREES"
	    #"java -jar $JAR -i $TEST $AUCJARPATH $TREES"
	    #"java -jar $JAR -i $TEST $AUCJARPATH $TREES"
	)
	
	COMPARES=(
	    "python compare.py datasets/$SET/test/results_$TARGET.db static/$SET/results_rdn.db"
	    #"python compare.py datasets/$SET/test/results_$TARGET.db static/$SET/results_rdn_noboost.db"
	    #"python compare.py datasets/$SET/test/results_$TARGET.db static/$SET/results_softmA0.5B-2.db"
	    #"python compare.py datasets/$SET/test/results_$TARGET.db static/$SET/results_softmA2B-10.db"
	    #"python compare.py datasets/$SET/test/results_$TARGET.db static/$SET/results_mln.db"
	    #"python compare.py datasets/$SET/test/results_$TARGET.db static/$SET/results_mln_mlnClause.db"
	    #"python compare.py datasets/$SET/test/results_$TARGET.db static/$SET/results_regression.db"
	)

	MOVES=(
	    "mv datasets/$SET/test/results_$TARGET.db static/$SET/results_rdn.db"
	    #"mv datasets/$SET/test/results_$TARGET.db static/$SET/results_rdn_noboost.db"
	    #"mv datasets/$SET/test/results_$TARGET.db static/$SET/results_softmA0.5B-2.db"
	    #"mv datasets/$SET/test/results_$TARGET.db static/$SET/results_softmA2B-10.db"
	    #"mv datasets/$SET/test/results_$TARGET.db static/$SET/results_mln.db"
	    #"mv datasets/$SET/test/results_$TARGET.db static/$SET/results_mln_mlnClause.db"
	    #"mv datasets/$SET/test/results_$TARGET.db static/$SET/results_regression.db"
	)
	
	rm -rf "datasets/$SET/train/models/*"
	
	for ((i=0; i<${#NAMES[*]}; i++)); do
	    printf "${LIGHTBLUE}Testing $e ${NAMES[i]}...${NC}\n"

	    # Side note: performing random walks by replacing modes file:
	    #echo "setParam: maxTreeDepth=3." > datasets/Cora/cora_bk.txt
	    #echo "setParam: nodeSize=2." >> datasets/Cora/cora_bk.txt
	    #python walker2.py -r Cora.mayukh | grep "mode:" >> datasets/Cora/cora_bk.txt
	    # This replaces the background modes with modes generated by walking random paths.
	    
	    ${TRAINS[i]} > trainlog.txt
	    #echo ${TRAINS[i]}
	    echo "${SET}: ${NAMES[i]}" >> Results.txt
	    grep "% Total learning time" trainlog.txt >> Results.txt
	    grep "% Total learning time" trainlog.txt | cut -d ':' -f 2 >> LEARNING.txt
	    #echo ${TRAINS[i]} >> Results.txt
	    if [[ "$?" = 0 ]]; then
		printf " - ${LIGHTGREEN}Training completed without errors.${NC}\n"
		${TESTS[i]} > testlog.txt
		#echo ${TESTS[i]}
		#echo ${TESTS[i]} >> Results.txt
		if [[ "$?" = 0 ]]; then
		    printf " - ${LIGHTGREEN}Testing completed without errors.${NC}\n"
		    if [[ $UPDATE = 'true' ]]; then
			echo "Updating baseline scores, results will not be calculated."
			${MOVES[i]}
		    else
			grep "% Total inference time" testlog.txt >> Results.txt
			grep "% Total inference time" testlog.txt | cut -d ':' -f 2 >> INFERENCE.txt
			# Instead of using the compare function, just keep track of AUC ROC / AUC PR
			grep "%   AUC ROC" testlog.txt >> Results.txt
			grep "%   AUC ROC" testlog.txt | cut -d '=' -f 2 >> roc.txt
			grep "%   AUC PR" testlog.txt >> Results.txt
			grep "%   AUC PR" testlog.txt | cut -d '=' -f 2 >> pr.txt
			#${COMPARES[i]} >> Results.txt
		    fi
		else
		    printf " - ${RED}Errors encountered during testing for ${NAMES[i]}.${NC}\n"
		    echo "Errors encountered during testing for ${NAMES[i]}." >> Results.txt
		fi
	    else
		printf " - ${RED}Errors encountered during training for ${NAMES[i]}.${NC}\n"
		echo "Errors encountered during training for ${NAMES[i]}." >> Results.txt
	    fi
	    printf "\n" >> Results.txt
	    printf "\n"
	done
    done
done

deleteIfExists trainlog.txt
deleteIfExists testlog.txt
deleteIfExists precomputed.txt
deleteIfExists recomputed.txt

exit 0

#if [[ -e datasets/Father/test/results_father.db ]]; then
# check if the file is the same as static/Father/results_father.db
#    X=$(md5sum datasets/Father/test/results_father.db | cut -f 1 -d ' ')
#    Y=$(md5sum static/Father/results_father.db | cut -f 1 -d ' ')
#    if [[ "$X" = "$Y" ]]; then
#	printf "${LIGHTGREEN}Success${NC}\n"
#    else
#	printf "${RED}Error, outcome does not match expected outcome.${NC}\n"
#	NUMBEROFERRORS=$((NUMBEROFERRORS+1))
#    fi
#fi
